{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230f9f63-acdb-4538-9d44-d9e3d24c8327",
   "metadata": {},
   "source": [
    "# Market Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26434dad1a98e76e",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore the Market Basket Analysis dataset, which can be downloaded from Kaggle using the following link: Market Basket Analysis Dataset.\n",
    "\n",
    "We will start by performing data cleaning to ensure the dataset is ready for analysis. Next, we will extract frequent itemsets using the FP-Growth algorithm, a powerful tool for mining frequent patterns in large datasets. Additionally, we will analyze the frequent itemsets on a per-country basis, enabling us to uncover unique purchasing patterns and trends across different regions.\n",
    "\n",
    "By the end of this tutorial, you will have a solid understanding of how to preprocess transactional data and use the FP-Growth algorithm to gain valuable insights into customer purchasing behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62da35-ddc7-4cd7-9ff1-4864bc25e3d7",
   "metadata": {},
   "source": [
    "Before we begin, we have to install some pandas and mlxtend to run queries on the data and run the FPGrowth algorithm.\n",
    "\n",
    "To install pandas and mlxtend with pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae11d8-45af-4ad3-8434-8c7167bd4e2d",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install pandas\n",
    "pip install mlxtend==0.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967a0ca-5dde-4114-b008-f4d5b4493b67",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad736c-3d6c-4055-b4ea-6b2f8fe0622a",
   "metadata": {},
   "source": [
    "Once the libraries have been installed, we can start to import data into our program. There are multiple ways of importing the data such as using the Kaggle API or downloading the csv file. In this tutorial, we will be using the downloaded folder provided in kaggle and renaming the folder to `data`. The file path is shown below in the variable `FILE` and can be modified to your liking.\n",
    "\n",
    "When importing the csv file with pandas, there is some inconsistent data present as a result of poor formatting. Since only a small number of rows are affected by this, we decided to just skip the rows and still retain a large portion of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c83a5-d933-49cf-9b1e-8b4d8c420d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:39.516319Z",
     "start_time": "2024-12-08T20:00:38.867113Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"./data/Assignment-1_Data.csv\"\n",
    "\n",
    "data = pd.read_csv(FILE, sep=\";\", on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033d9fe-37b5-40af-8539-8fee954915f1",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ed3ae-7b52-48e3-a408-9831d26a86ad",
   "metadata": {},
   "source": [
    "In this step, we will clean the dataset by focusing on the columns that are most relevant to our analysis: `BillNo`, `Itemname`, and `Country`. All other columns will be dropped, as they are not necessary for the insights we aim to extract. Next, we will remove any entries where the data is incomplete, specifically rows where `BillNo`, `Itemname`, or `Country` are missing. These incomplete records can introduce inconsistencies or inaccuracies in our analysis, so it is important to exclude them. Additionally, we will clean up the Itemname column by removing any leading or trailing whitespace to ensure consistency and accuracy when analyzing item names. These preprocessing steps will ensure that the dataset is clean, focused, and ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7670df0d15d877b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:42.910576Z",
     "start_time": "2024-12-08T20:00:42.800444Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_keep = ['BillNo', 'Itemname', 'Country']\n",
    "\n",
    "data = data[columns_to_keep]\n",
    "\n",
    "#Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data['Itemname'] = data['Itemname'].str.strip()\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae48d8727dc7f1",
   "metadata": {},
   "source": [
    "To analyze transactions across different regions, we separate the data by country. This is achieved by creating a dictionary called `country_datas`, which leverages the groupby function to group all rows based on the `Country` column. Each country serves as a key in the dictionary, with its corresponding value being a subset of the data containing only transactions for that specific country. However, there are transactions with a `Country` of `Undefined` which we will exclude from our data. To maintain accuracy of our results, we only keep the countries where there are at least 1000 rows of transactional data.\n",
    "\n",
    "Finally, a quick preview of the transactions for each country is displayed which can be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f853d44553372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:45.652112Z",
     "start_time": "2024-12-08T20:00:45.601318Z"
    }
   },
   "outputs": [],
   "source": [
    "country_datas = {country: data for country, data in data.groupby('Country')}\n",
    "    \n",
    "del country_datas[\"Unspecified\"]\n",
    "\n",
    "# Keep countries with more than 2000 rows of transaction details \n",
    "country_datas = {key: value for key, value in country_datas.items() if value.shape[0] > 1000}\n",
    "\n",
    "for country, data in country_datas.items():\n",
    "        print(f\"Data for {country}:\")\n",
    "        print(data.head()) \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41452519bee441c9",
   "metadata": {},
   "source": [
    "In our current dataset, we have `country_data`, which contains transactions for each country, organized by `BillNo`. To prepare the data for frequent itemset mining, we need to group all transactions sharing the same BillNo into a single transaction. This ensures that all items purchased together in the same transaction are treated as a single unit. We achieve this by using the `groupby` function to aggregate the items by `BillNo`, effectively combining them into grouped transactions. \n",
    "\n",
    "Examples of our transactions can be ran in the following print statements to have a close look. After this transformation, out transactions data for each country is now ready for the `TransactionEncoder` in `mlxtend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a6297f1e4a89a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:48.349847Z",
     "start_time": "2024-12-08T20:00:48.069867Z"
    }
   },
   "outputs": [],
   "source": [
    "country_transactions = {}\n",
    "\n",
    "for country, data in country_datas.items():\n",
    "    country_transactions[country] = data.groupby(['BillNo'])['Itemname'].apply(lambda x: ','.join(x)).reset_index()\n",
    "        \n",
    "for country, transactions in country_transactions.items():\n",
    "    transactions.drop(columns=['BillNo'], inplace=True)\n",
    "    transactions.rename(columns={'Itemname': 'Items'}, inplace=True)\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    print(f\"Transactions for {country}:\")\n",
    "    print(transactions.head()) \n",
    "    print(\"\\n\")\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    country_transactions[country] = transactions['Items'].apply(lambda x: x.split(',')).tolist()\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    print(f\"Transactions for {country}:\")\n",
    "    print(transactions[0]) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c51d98-aed7-44b3-bdac-08c9db3f0f7b",
   "metadata": {},
   "source": [
    "# Generating Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931411d38633657",
   "metadata": {},
   "source": [
    "To perform frequent itemset mining using the FP-Growth algorithm, we first prepared our data by transforming the `country_transactions` dictionary into a one-hot encoded format. The FP-Growth functions from the `mlxtend` library expect the input to be a binary matrix where each row represents a transaction, and columns represent the presence or absence of items. Using `TransactionEncoder` from `mlxtend.preprocessing`, we transformed each country's transaction data into a Pandas DataFrame with binary encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefff233771e699a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:52.016231Z",
     "start_time": "2024-12-08T20:00:51.248983Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "for country, transactions in country_transactions.items():\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    data = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    country_transactions[country] = data\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    print(f\"Transactions for {country}:\")\n",
    "    print(transactions.head())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05db855-13a8-4257-8561-50cad03dd6c3",
   "metadata": {},
   "source": [
    "Next, we applied the FP-Growth algorithm to extract frequent itemsets and association rules for each country's transactions. For a subset of countries, including United Kingdom, France, Germany, and others, we identified frequent itemsets with a minimum support threshold of 0.1, ensuring only the most relevant patterns are included. These frequent itemsets were then sorted by their support values to highlight the most common combinations of items. Additionally, we used the association_rules function to derive meaningful rules, filtering them based on a confidence threshold of 0.8. \n",
    "\n",
    "Finally, we displayed the frequent itemsets and association rules for each country to gain insights into region-specific shopping patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60710d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:55.312696Z",
     "start_time": "2024-12-08T20:00:55.143949Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "fq_itemsets = {}\n",
    "fq_rules = {}\n",
    "# Apply FP-Growth to each country's transactions\n",
    "for country, transactions in country_transactions.items():\n",
    "    #if country in {'United Kingdom', 'France', 'Germany', 'Australia', 'Austria', 'Bahrain', 'Belgium'}:\n",
    "    frequent_itemsets = fpgrowth(transactions, min_support=0.1, use_colnames=True)\n",
    "    top_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "    rules = association_rules(top_itemsets, metric='confidence', min_threshold=0.8)\n",
    "    fq_itemsets[country] = frequent_itemsets\n",
    "    fq_rules[country] = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63393884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:58.625097Z",
     "start_time": "2024-12-08T20:00:58.611879Z"
    }
   },
   "outputs": [],
   "source": [
    "for country, itemsets in fq_itemsets.items():\n",
    "    print(f\"Frequent itemsets for {country}\")\n",
    "    print(len(itemsets))\n",
    "    print(itemsets.sort_values(by='support', ascending=False).head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c67a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:01:14.598951Z",
     "start_time": "2024-12-08T20:01:14.579969Z"
    }
   },
   "outputs": [],
   "source": [
    "for country, rules in fq_rules.items():\n",
    "    print(f\"Association rules for {country}\")\n",
    "    print(len(rules))\n",
    "    print(rules.head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80b4d24b",
   "metadata": {},
   "source": [
    "To analyze the strength of association rules, we categorize them based on lift: rules with lift > 1 are positively correlated, and those with lift â‰¤ 1 are negatively correlated. Overall, most of the rules are positively correlated, which is a good sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, rules in fq_rules.items():\n",
    "    # Separate rules based on lift\n",
    "    positive_corr = rules[rules['lift'] > 1]\n",
    "    negative_corr = rules[rules['lift'] <= 1]\n",
    "    \n",
    "    # Print association rule correlation summary\n",
    "    print(f\"Association rules correlation for country: {country}\")\n",
    "    print(f\"Total Rules: {len(rules)}\")\n",
    "    print(f\"Positive correlation rules: {len(positive_corr)}\")\n",
    "    print(f\"Negative correlation rules: {len(negative_corr)}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279b615-d03a-4ebe-81f0-fa1ad5f35f69",
   "metadata": {},
   "source": [
    "# Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09761e6-dea7-48e5-96eb-4e9386e5dfad",
   "metadata": {},
   "source": [
    "The Apriori algorithm is another method for detecting frequent itemsets, leveraging the `apriori` and `association_rules` functions from the `mlxtend` library. In this analysis, we applied the Apriori algorithm to the same subset of countries and used the same minimum support threshold of 0.1. These itemsets were then sorted in descending order of support to identify the most common combinations of items. Additionally, we used the association_rules function to extract meaningful rules from the frequent itemsets, applying a minimum confidence threshold of 0.2 to filter the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbb333-7310-4e27-ab07-fd5ae00a9aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:01:53.460065Z",
     "start_time": "2024-12-08T20:01:53.457655Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "apriori_itemsets = {}\n",
    "apriori_rules = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9005c4c-c413-45b9-af08-1fe92f4cdd49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:01:55.316500Z",
     "start_time": "2024-12-08T20:01:55.221437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Apriori to each country's transactions\n",
    "for country, transactions in country_transactions.items():\n",
    "    #if country in {'United Kingdom', 'France', 'Germany', 'Australia', 'Austria', 'Bahrain', 'Belgium'}:\n",
    "    print(f\"Processing Apriori for {country}...\\n\")\n",
    "        \n",
    "    # Generate frequent itemsets using Apriori\n",
    "    frequent_itemsets = apriori(transactions, min_support=0.1, use_colnames=True)\n",
    "        \n",
    "    # Sort itemsets by support in descending order\n",
    "    top_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "        \n",
    "    # Generate association rules from the frequent itemsets\n",
    "    rules = association_rules(top_itemsets, metric='confidence', min_threshold=0.8)\n",
    "        \n",
    "    # Store results\n",
    "    apriori_itemsets[country] = frequent_itemsets\n",
    "    apriori_rules[country] = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7703634-6693-40d2-a7e9-4cfd171f4ed8",
   "metadata": {},
   "source": [
    "Finally, we stored and displayed the frequent itemsets and association rules for each country, providing insights into purchasing patterns based on the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bba84b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:01:58.167095Z",
     "start_time": "2024-12-08T20:01:58.153933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display results for frequent itemsets\n",
    "for country, itemsets in apriori_itemsets.items():\n",
    "    print(f\"Frequent itemsets for {country} (Apriori)\")\n",
    "    print(len(itemsets))\n",
    "    print(itemsets.sort_values(by='support', ascending=False).head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970563b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:02:25.561094Z",
     "start_time": "2024-12-08T20:02:25.545311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display results for association rules\n",
    "for country, rules in apriori_rules.items():\n",
    "    print(f\"Association rules for {country} (Apriori)\")\n",
    "    print(len(rules))\n",
    "    print(rules.head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9cb85-0d1d-4fc3-898c-a7ad26725ec4",
   "metadata": {},
   "source": [
    "Once we run through all the calculations, we observe that the results of the Apriori algorithm matches the results obtained by FP Growth for all countries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79083760",
   "metadata": {},
   "source": [
    "# KNOWLEDGE REPRESENTATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8321ee",
   "metadata": {},
   "source": [
    "At the final stage of Knowledge Discovery in Databases (KDD), \n",
    "the knowledge representation is captured in a graph database, \n",
    "which enables better insight and visualization of the discovered patterns. \n",
    "In this step, we store the association rules discovered by algorithms such as \n",
    "Apriori in a Neo4j graph database.\n",
    "\n",
    "In the provided code, a connection to the Neo4j database is established, \n",
    "and a dynamic structure is created for storing countries, \n",
    "association rules (antecedents and consequents), and their relationships, \n",
    "such as `HAS_RULE` and `RESULTS_INTO`. \n",
    "This allows you to represent associations between items (antecedents and consequents) \n",
    "for specific countries with their \n",
    "corresponding support, confidence, and lift values.\n",
    "\n",
    "The graph is visualized using the `networkx` library and the `matplotlib` library, \n",
    "which provides a clear and interactive way \n",
    "to explore the relationships. Each rule is visualized as a directed edge, \n",
    "with the support, confidence, and lift values labeled on the edges, \n",
    "giving a visual overview of the strength and reliability of each rule. \n",
    "This representation helps in understanding the relationships between items in a country \n",
    "and can further be used for different use cases like recommendation systems or market basket analysis.\n",
    "\n",
    "Here's a breakdown of the core steps involved in the code:\n",
    "\n",
    "1. **Neo4j Graph Database Connection**: Establishing a connection to a Neo4j database where the rules will be stored.\n",
    "2. **Storing Rules**: The `create_rule` function dynamically creates country nodes, antecedent nodes, and consequent nodes, and links them with relationships such as `HAS_RULE` and `RESULTS_INTO`, with properties for support, confidence, and lift.\n",
    "3. **Visualizing the Graph**: The `visualize_graph` function generates a visual representation of the graph using `networkx`, highlighting the rules between antecedents and consequents.\n",
    "4. **Clearing the Database**: Before creating new rules, the `clear_database` function deletes any existing rules and nodes, ensuring the graph remains up-to-date with the latest rules.\n",
    "\n",
    "This step completes the KDD pipeline by transforming raw data into actionable insights represented as knowledge in a graph structure, ready for further analysis or decision-making.\n",
    "\n",
    "### Why this is better than traditional storage:\n",
    "- **Efficient Relationship Modeling**: Unlike traditional relational databases, which struggle with complex, many-to-many relationships, Neo4j is optimized for handling and querying connected data. It allows for intuitive representation and querying of relationships, making it easier to uncover hidden patterns and associations between items.\n",
    "  \n",
    "- **Scalability and Flexibility**: As the data grows, graph databases like Neo4j efficiently scale and adapt to complex structures, without the need for expensive joins or cumbersome relational tables. This flexibility is ideal for dynamic and evolving data, like association rules, where relationships are central to the analysis.\n",
    "\n",
    "These features enable faster, more efficient querying and provide deeper insights into the data compared to traditional row-based storage systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c17b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Neo4j connection details\n",
    "uri = \"neo4j+s://27f471e5.databases.neo4j.io\"  # Your Aura instance URI\n",
    "username = \"neo4j\"  # Your Neo4j username\n",
    "password = \"EI5gKZQ6XT0y4bMRMej9orSxnkEP-Xc77lqZLH6Hkac\"  # Your Neo4j password\n",
    "\n",
    "# Initialize the driver\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "# Function to create a country node (dynamic)\n",
    "def create_country(tx, country_name):\n",
    "    tx.run(\"MERGE (ct:Country {name: $country_name})\", country_name=country_name)\n",
    "\n",
    "# Function to create rules based on the given antecedent, consequent, and properties\n",
    "def create_rule(tx, country_name, antecedent_items, consequent_items, support, confidence, lift):\n",
    "    tx.run(\n",
    "        \"\"\"\n",
    "        MERGE (a:Antecedent {items: $antecedent_items})\n",
    "        MERGE (c:Consequent {items: $consequent_items})\n",
    "        WITH a, c\n",
    "        MATCH (ct:Country {name: $country_name})\n",
    "        MERGE (ct)-[:HAS_RULE]->(a)-[:RESULTS_INTO {support: $support, confidence: $confidence, lift: $lift}]->(c)\n",
    "        \"\"\",\n",
    "        antecedent_items=antecedent_items,\n",
    "        consequent_items=consequent_items,\n",
    "        support=support,\n",
    "        confidence=confidence,\n",
    "        lift=lift,\n",
    "        country_name=country_name\n",
    "    )\n",
    "\n",
    "# Function to visualize the graph and save as an image\n",
    "def visualize_graph(G, filename=\"graph.png\"):\n",
    "    pos = nx.spring_layout(G)  # Positioning nodes\n",
    "    plt.figure(figsize=(12, 12))  # Adjust figure size\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=3000, font_size=10, font_weight='bold', edge_color='gray')\n",
    "    \n",
    "    # Draw edge labels (support, confidence, lift)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'support')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    \n",
    "    # Save the graph as an image\n",
    "    plt.title(\"Association Rules Graph\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "def clear_database(tx):\n",
    "    # Delete HAS_RULE relationships\n",
    "    tx.run(\"MATCH (a)-[re:HAS_RULE]-(b) DELETE re\")\n",
    "    # Delete RESULTS_INTO relationships\n",
    "    tx.run(\"MATCH (a)-[re:RESULTS_INTO]-(b) DELETE re\")\n",
    "    # Delete all nodes\n",
    "    tx.run(\"MATCH (n) DELETE n\")\n",
    "    \n",
    "# Function to fetch rules for a specific country\n",
    "def fetch_rules(tx, country_name):\n",
    "    query = \"\"\"\n",
    "    MATCH (ct:Country {name: $country_name})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "    RETURN ct, a, r1, r2, c\n",
    "    \"\"\"\n",
    "    result = tx.run(query, country_name=country_name)\n",
    "    return [\n",
    "        {\"antecedent\": record[\"a\"].get(\"items\"), \"consequent\": record[\"c\"].get(\"items\"), \n",
    "         \"support\": record[\"r2\"].get(\"support\"), \"confidence\": record[\"r2\"].get(\"confidence\"), \"lift\": record[\"r2\"].get(\"lift\")}\n",
    "        for record in result\n",
    "    ]\n",
    "\n",
    "# Function to create a graph representation of the rules\n",
    "def create_graph(rules):\n",
    "    G = nx.DiGraph()  # Directed graph\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    for rule in rules:\n",
    "        antecedent = str(rule['antecedent'])\n",
    "        consequent = str(rule['consequent'])\n",
    "        G.add_node(antecedent, type='Antecedent')\n",
    "        G.add_node(consequent, type='Consequent')\n",
    "        G.add_edge(\n",
    "            antecedent, consequent,\n",
    "            support=rule['support'], confidence=rule['confidence'], lift=rule['lift']\n",
    "        )\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Function to visualize the graph and save it as an image\n",
    "def visualize_graph(G, filename=\"graph.png\"):\n",
    "    pos = nx.spring_layout(G)  # Positioning nodes\n",
    "    plt.figure(figsize=(12, 12))  # Adjust figure size\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=3000, font_size=10, font_weight='bold', edge_color='gray')\n",
    "    \n",
    "    # Draw edge labels (support, confidence, lift)\n",
    "    edge_labels = {\n",
    "        (u, v): f\"Supp: {d['support']:.2f}, Conf: {d['confidence']:.2f}, Lift: {d['lift']:.2f}\"\n",
    "        for u, v, d in G.edges(data=True)\n",
    "    }\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    \n",
    "    # Save the graph as an image\n",
    "    plt.title(\"Association Rules Graph\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    # Clear the database in the specified sequence\n",
    "    session.execute_write(clear_database)\n",
    "    print(\"All nodes and relationships deleted in sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ce321",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, rules in fq_rules.items():\n",
    "    print(f\"Association rules for {country} in graph database - Neo4j\")\n",
    "    \n",
    "    # Initialize the list to hold the association rules\n",
    "    association_rules = []\n",
    "    \n",
    "    rules = rules.head(5)\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in rules.iterrows():\n",
    "        # Create a dictionary for each rule and append it to the list\n",
    "        rule = {\n",
    "            'antecedents': row['antecedents'],\n",
    "            'consequents': row['consequents'],\n",
    "            'support': row['support'],\n",
    "            'confidence': row['confidence'],\n",
    "            'lift': row['lift']\n",
    "        }\n",
    "        association_rules.append(rule)\n",
    "    \n",
    "    # Display the result\n",
    "    print(association_rules)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    country_name = country\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # Create the country node\n",
    "        session.execute_write(create_country, country_name)\n",
    "    \n",
    "        # Loop through the association_rules array and create each rule\n",
    "        for rule in association_rules:\n",
    "            antecedents = list(rule['antecedents'])\n",
    "            consequents = list(rule['consequents'])\n",
    "            support = rule['support']\n",
    "            confidence = rule['confidence']\n",
    "            lift = rule['lift']\n",
    "            \n",
    "            # Create the rule in the database with the dynamic country name\n",
    "            session.execute_write(create_rule, country_name, antecedents, consequents, support, confidence, lift)\n",
    "        \n",
    "        # Fetch rules for the specified country\n",
    "        rules = session.execute_read(fetch_rules, country_name)\n",
    "    \n",
    "        # Create a graph from the fetched rules\n",
    "        G = create_graph(rules)\n",
    "        \n",
    "        # Visualize the graph and save the image\n",
    "        visualize_graph(G, filename=f\"association_rules_{country_name}.png\")\n",
    "        \n",
    "        print(f\"Graph image saved as 'association_rules_{country_name}.png'\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a289c715",
   "metadata": {},
   "source": [
    "Graph images are particularly not getting saved very well, and this is something we couldn't handle effectively. However, you can easily view the graph database's contents and experiment with it directly by following these steps:\n",
    "\n",
    "**Steps to explore the graph data:**\n",
    "\n",
    "1. **Visit the Neo4j Browser** at [this link](https://27f471e5.databases.neo4j.io/browser/).\n",
    "   \n",
    "2. **Login details:**\n",
    "   - **Username**: `neo4j`\n",
    "   - **Password**: `EI5gKZQ6XT0y4bMRMej9orSxnkEP-Xc77lqZLH6Hkac`\n",
    "\n",
    "3. **Wait for 60 seconds** before connecting to the database, or you can log in to [Neo4j Console](https://console.neo4j.io) to ensure that your Aura instance is available and ready to use.\n",
    "\n",
    "4. **Use the following Cypher queries** for each country to view their respective association rule graphs. These queries will return nodes and relationships representing the association rules:\n",
    "\n",
    "### Queries for Graph Representation:\n",
    "\n",
    "- **Australia**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'Australia'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "- **Belgium**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'Belgium'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "- **France**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'France'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "- **Germany**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'Germany'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "- **Netherlands**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'Netherlands'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "- **Norway**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'Norway'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "- **Portugal**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'Portugal'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "- **Spain**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'Spain'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "- **Switzerland**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'Switzerland'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "- **United Kingdom**:\n",
    "  ```cypher\n",
    "  MATCH (ct:Country {name: 'United Kingdom'})-[r1:HAS_RULE]->(a:Antecedent)-[r2:RESULTS_INTO]->(c:Consequent)\n",
    "  RETURN ct, a, r1, r2, c\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "These queries will help you visualize the relationship between `Country`, `Antecedent`, and `Consequent` nodes, showing how the association rules are structured for each country.\n",
    "\n",
    "### Potential Use Cases for the Knowledge Graphs:\n",
    "These knowledge graphs could be utilized for various purposes such as:\n",
    "\n",
    "- **Trend Analysis**: Compare association rules across different countries.\n",
    "- **Product Recommendations**: Discover frequently bought items in different countries and make recommendations.\n",
    "- **Data-Driven Decisions**: Utilize association rules to inform strategic decisions.\n",
    "- **Visual Insights**: Present graph-based insights for stakeholders to interpret complex data visually.\n",
    "\n",
    "By following the above steps, you can explore and analyze the data directly in Neo4j's browser and leverage it for deeper analysis or further use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the driver\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cacba1f63d0a9690f1901f7a6a70bca9f28ac070c74075aae2450729060a64a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
