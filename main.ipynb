{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230f9f63-acdb-4538-9d44-d9e3d24c8327",
   "metadata": {},
   "source": [
    "# Market Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26434dad1a98e76e",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore the Market Basket Analysis dataset, which can be downloaded from Kaggle using the following link: Market Basket Analysis Dataset.\n",
    "\n",
    "We will start by performing data cleaning to ensure the dataset is ready for analysis. Next, we will extract frequent itemsets using the FP-Growth algorithm, a powerful tool for mining frequent patterns in large datasets. Additionally, we will analyze the frequent itemsets on a per-country basis, enabling us to uncover unique purchasing patterns and trends across different regions.\n",
    "\n",
    "By the end of this tutorial, you will have a solid understanding of how to preprocess transactional data and use the FP-Growth algorithm to gain valuable insights into customer purchasing behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62da35-ddc7-4cd7-9ff1-4864bc25e3d7",
   "metadata": {},
   "source": [
    "Before we begin, we have to install some pandas and mlxtend to run queries on the data and run the FPGrowth algorithm.\n",
    "\n",
    "To install pandas and mlxtend with pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae11d8-45af-4ad3-8434-8c7167bd4e2d",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install pandas\n",
    "pip install mlxtend==0.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967a0ca-5dde-4114-b008-f4d5b4493b67",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad736c-3d6c-4055-b4ea-6b2f8fe0622a",
   "metadata": {},
   "source": [
    "Once the libraries have been installed, we can start to import data into our program. There are multiple ways of importing the data such as using the Kaggle API or downloading the csv file. In this tutorial, we will be using the downloaded folder provided in kaggle and renaming the folder to `data`. The file path is shown below in the variable `FILE` and can be modified to your liking.\n",
    "\n",
    "When importing the csv file with pandas, there is some inconsistent data present as a result of poor formatting. Since only a small number of rows are affected by this, we decided to just skip the rows and still retain a large portion of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c83a5-d933-49cf-9b1e-8b4d8c420d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:39.516319Z",
     "start_time": "2024-12-08T20:00:38.867113Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"./data/Assignment-1_Data.csv\"\n",
    "\n",
    "data = pd.read_csv(FILE, sep=\";\", on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033d9fe-37b5-40af-8539-8fee954915f1",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ed3ae-7b52-48e3-a408-9831d26a86ad",
   "metadata": {},
   "source": [
    "In this step, we will clean the dataset by focusing on the columns that are most relevant to our analysis: `BillNo`, `Itemname`, and `Country`. All other columns will be dropped, as they are not necessary for the insights we aim to extract. Next, we will remove any entries where the data is incomplete, specifically rows where `BillNo`, `Itemname`, or `Country` are missing. These incomplete records can introduce inconsistencies or inaccuracies in our analysis, so it is important to exclude them. Additionally, we will clean up the Itemname column by removing any leading or trailing whitespace to ensure consistency and accuracy when analyzing item names. These preprocessing steps will ensure that the dataset is clean, focused, and ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7670df0d15d877b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:42.910576Z",
     "start_time": "2024-12-08T20:00:42.800444Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_keep = ['BillNo', 'Itemname', 'Country']\n",
    "\n",
    "data = data[columns_to_keep]\n",
    "\n",
    "#Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data['Itemname'] = data['Itemname'].str.strip()\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae48d8727dc7f1",
   "metadata": {},
   "source": [
    "To analyze transactions across different regions, we separate the data by country. This is achieved by creating a dictionary called `country_datas`, which leverages the groupby function to group all rows based on the `Country` column. Each country serves as a key in the dictionary, with its corresponding value being a subset of the data containing only transactions for that specific country. However, there are transactions with a `Country` of `Undefined` which we will exclude from our data. To maintain accuracy of our results, we only keep the countries where there are at least 1000 rows of transactional data.\n",
    "\n",
    "Finally, a quick preview of the transactions for each country is displayed which can be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f853d44553372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:45.652112Z",
     "start_time": "2024-12-08T20:00:45.601318Z"
    }
   },
   "outputs": [],
   "source": [
    "country_datas = {country: data for country, data in data.groupby('Country')}\n",
    "    \n",
    "del country_datas[\"Unspecified\"]\n",
    "\n",
    "# Keep countries with more than 2000 rows of transaction details \n",
    "country_datas = {key: value for key, value in country_datas.items() if value.shape[0] > 1000}\n",
    "\n",
    "for country, data in country_datas.items():\n",
    "        print(f\"Data for {country}:\")\n",
    "        print(data.head()) \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41452519bee441c9",
   "metadata": {},
   "source": [
    "In our current dataset, we have `country_data`, which contains transactions for each country, organized by `BillNo`. To prepare the data for frequent itemset mining, we need to group all transactions sharing the same BillNo into a single transaction. This ensures that all items purchased together in the same transaction are treated as a single unit. We achieve this by using the `groupby` function to aggregate the items by `BillNo`, effectively combining them into grouped transactions. \n",
    "\n",
    "Examples of our transactions can be ran in the following print statements to have a close look. After this transformation, out transactions data for each country is now ready for the `TransactionEncoder` in `mlxtend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a6297f1e4a89a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:48.349847Z",
     "start_time": "2024-12-08T20:00:48.069867Z"
    }
   },
   "outputs": [],
   "source": [
    "country_transactions = {}\n",
    "\n",
    "for country, data in country_datas.items():\n",
    "    country_transactions[country] = data.groupby(['BillNo'])['Itemname'].apply(lambda x: ','.join(x)).reset_index()\n",
    "        \n",
    "for country, transactions in country_transactions.items():\n",
    "    transactions.drop(columns=['BillNo'], inplace=True)\n",
    "    transactions.rename(columns={'Itemname': 'Items'}, inplace=True)\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    print(f\"Transactions for {country}:\")\n",
    "    print(transactions.head()) \n",
    "    print(\"\\n\")\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    country_transactions[country] = transactions['Items'].apply(lambda x: x.split(',')).tolist()\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    print(f\"Transactions for {country}:\")\n",
    "    print(transactions[0]) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c51d98-aed7-44b3-bdac-08c9db3f0f7b",
   "metadata": {},
   "source": [
    "# Generating Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931411d38633657",
   "metadata": {},
   "source": [
    "To perform frequent itemset mining using the FP-Growth algorithm, we first prepared our data by transforming the `country_transactions` dictionary into a one-hot encoded format. The FP-Growth functions from the `mlxtend` library expect the input to be a binary matrix where each row represents a transaction, and columns represent the presence or absence of items. Using `TransactionEncoder` from `mlxtend.preprocessing`, we transformed each country's transaction data into a Pandas DataFrame with binary encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefff233771e699a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:52.016231Z",
     "start_time": "2024-12-08T20:00:51.248983Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "for country, transactions in country_transactions.items():\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    data = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    country_transactions[country] = data\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    print(f\"Transactions for {country}:\")\n",
    "    print(transactions.head())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05db855-13a8-4257-8561-50cad03dd6c3",
   "metadata": {},
   "source": [
    "Next, we applied the FP-Growth algorithm to extract frequent itemsets and association rules for each country's transactions. For a subset of countries, including United Kingdom, France, Germany, and others, we identified frequent itemsets with a minimum support threshold of 0.1, ensuring only the most relevant patterns are included. These frequent itemsets were then sorted by their support values to highlight the most common combinations of items. Additionally, we used the association_rules function to derive meaningful rules, filtering them based on a confidence threshold of 0.8. \n",
    "\n",
    "Finally, we displayed the frequent itemsets and association rules for each country to gain insights into region-specific shopping patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60710d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:55.312696Z",
     "start_time": "2024-12-08T20:00:55.143949Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "fq_itemsets = {}\n",
    "fq_rules = {}\n",
    "# Apply FP-Growth to each country's transactions\n",
    "for country, transactions in country_transactions.items():\n",
    "    #if country in {'United Kingdom', 'France', 'Germany', 'Australia', 'Austria', 'Bahrain', 'Belgium'}:\n",
    "    frequent_itemsets = fpgrowth(transactions, min_support=0.1, use_colnames=True)\n",
    "    top_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "    rules = association_rules(top_itemsets, metric='confidence', min_threshold=0.8)\n",
    "    fq_itemsets[country] = frequent_itemsets\n",
    "    fq_rules[country] = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63393884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:00:58.625097Z",
     "start_time": "2024-12-08T20:00:58.611879Z"
    }
   },
   "outputs": [],
   "source": [
    "for country, itemsets in fq_itemsets.items():\n",
    "    print(f\"Frequent itemsets for {country}\")\n",
    "    print(len(itemsets))\n",
    "    print(itemsets.sort_values(by='support', ascending=False).head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c67a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:01:14.598951Z",
     "start_time": "2024-12-08T20:01:14.579969Z"
    }
   },
   "outputs": [],
   "source": [
    "for country, rules in fq_rules.items():\n",
    "    print(f\"Association rules for {country}\")\n",
    "    print(len(rules))\n",
    "    print(rules.head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80b4d24b",
   "metadata": {},
   "source": [
    "To analyze the strength of association rules, we categorize them based on lift: rules with lift > 1 are positively correlated, and those with lift ≤ 1 are negatively correlated. Overall, most of the rules are positively correlated, which is a good sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, rules in fq_rules.items():\n",
    "    # Separate rules based on lift\n",
    "    positive_corr = rules[rules['lift'] > 1]\n",
    "    negative_corr = rules[rules['lift'] <= 1]\n",
    "    \n",
    "    # Print association rule correlation summary\n",
    "    print(f\"Association rules correlation for country: {country}\")\n",
    "    print(f\"Total Rules: {len(rules)}\")\n",
    "    print(f\"Positive correlation rules: {len(positive_corr)}\")\n",
    "    print(f\"Negative correlation rules: {len(negative_corr)}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279b615-d03a-4ebe-81f0-fa1ad5f35f69",
   "metadata": {},
   "source": [
    "# Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09761e6-dea7-48e5-96eb-4e9386e5dfad",
   "metadata": {},
   "source": [
    "The Apriori algorithm is another method for detecting frequent itemsets, leveraging the `apriori` and `association_rules` functions from the `mlxtend` library. In this analysis, we applied the Apriori algorithm to the same subset of countries and used the same minimum support threshold of 0.1. These itemsets were then sorted in descending order of support to identify the most common combinations of items. Additionally, we used the association_rules function to extract meaningful rules from the frequent itemsets, applying a minimum confidence threshold of 0.2 to filter the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbb333-7310-4e27-ab07-fd5ae00a9aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:01:53.460065Z",
     "start_time": "2024-12-08T20:01:53.457655Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "apriori_itemsets = {}\n",
    "apriori_rules = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9005c4c-c413-45b9-af08-1fe92f4cdd49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:01:55.316500Z",
     "start_time": "2024-12-08T20:01:55.221437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Apriori to each country's transactions\n",
    "for country, transactions in country_transactions.items():\n",
    "    #if country in {'United Kingdom', 'France', 'Germany', 'Australia', 'Austria', 'Bahrain', 'Belgium'}:\n",
    "    print(f\"Processing Apriori for {country}...\\n\")\n",
    "        \n",
    "    # Generate frequent itemsets using Apriori\n",
    "    frequent_itemsets = apriori(transactions, min_support=0.1, use_colnames=True)\n",
    "        \n",
    "    # Sort itemsets by support in descending order\n",
    "    top_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "        \n",
    "    # Generate association rules from the frequent itemsets\n",
    "    rules = association_rules(top_itemsets, metric='confidence', min_threshold=0.8)\n",
    "        \n",
    "    # Store results\n",
    "    apriori_itemsets[country] = frequent_itemsets\n",
    "    apriori_rules[country] = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7703634-6693-40d2-a7e9-4cfd171f4ed8",
   "metadata": {},
   "source": [
    "Finally, we stored and displayed the frequent itemsets and association rules for each country, providing insights into purchasing patterns based on the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bba84b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:01:58.167095Z",
     "start_time": "2024-12-08T20:01:58.153933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display results for frequent itemsets\n",
    "for country, itemsets in apriori_itemsets.items():\n",
    "    print(f\"Frequent itemsets for {country} (Apriori)\")\n",
    "    print(len(itemsets))\n",
    "    print(itemsets.sort_values(by='support', ascending=False).head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3970563b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:02:25.561094Z",
     "start_time": "2024-12-08T20:02:25.545311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display results for association rules\n",
    "for country, rules in apriori_rules.items():\n",
    "    print(f\"Association rules for {country} (Apriori)\")\n",
    "    print(len(rules))\n",
    "    print(rules.head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9cb85-0d1d-4fc3-898c-a7ad26725ec4",
   "metadata": {},
   "source": [
    "Once we run through all the calculations, we observe that the results of the Apriori algorithm matches the results obtained by FP Growth for all countries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "cacba1f63d0a9690f1901f7a6a70bca9f28ac070c74075aae2450729060a64a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
