{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26434dad1a98e76e",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore the Market Basket Analysis dataset, which can be downloaded from Kaggle using the following link: Market Basket Analysis Dataset.\n",
    "\n",
    "We will start by performing data cleaning to ensure the dataset is ready for analysis. Next, we will extract frequent itemsets using the FP-Growth algorithm, a powerful tool for mining frequent patterns in large datasets. Additionally, we will analyze the frequent itemsets on a per-country basis, enabling us to uncover unique purchasing patterns and trends across different regions.\n",
    "\n",
    "By the end of this tutorial, you will have a solid understanding of how to preprocess transactional data and use the FP-Growth algorithm to gain valuable insights into customer purchasing behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62da35-ddc7-4cd7-9ff1-4864bc25e3d7",
   "metadata": {},
   "source": [
    "Before we begin, we have to install some pandas and mlxtend to run queries on the data and run the FPGrowth algorithm.\n",
    "\n",
    "To install pandas and mlxtend with pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae11d8-45af-4ad3-8434-8c7167bd4e2d",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install pandas\n",
    "pip install mlxtend==0.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967a0ca-5dde-4114-b008-f4d5b4493b67",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad736c-3d6c-4055-b4ea-6b2f8fe0622a",
   "metadata": {},
   "source": [
    "Once the libraries have been installed, we can start to import data into our program. There are multiple ways of importing the data such as using the Kaggle API or downloading the csv file. In this tutorial, we will be using the downloaded folder provided in kaggle and renaming the folder to `data`. The file path is shown below in the variable `FILE` and can be modified to your liking.\n",
    "\n",
    "When importing the csv file with pandas, there is some inconsistent data present as a result of poor formatting. Since only a small number of rows are affected by this, we decided to just skip the rows and still retain a large portion of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304c83a5-d933-49cf-9b1e-8b4d8c420d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:03.467525Z",
     "start_time": "2024-12-07T01:42:03.077807Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"./data/Assignment-1_Data.csv\"\n",
    "\n",
    "data = pd.read_csv(FILE, sep=\";\", on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033d9fe-37b5-40af-8539-8fee954915f1",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87ed3ae-7b52-48e3-a408-9831d26a86ad",
   "metadata": {},
   "source": [
    "In this step, we will clean the dataset by focusing on the columns that are most relevant to our analysis: `BillNo`, `Itemname`, and `Country`. All other columns will be dropped, as they are not necessary for the insights we aim to extract. Next, we will remove any entries where the data is incomplete, specifically rows where `BillNo`, `Itemname`, or `Country` are missing. These incomplete records can introduce inconsistencies or inaccuracies in our analysis, so it is important to exclude them. Additionally, we will clean up the Itemname column by removing any leading or trailing whitespace to ensure consistency and accuracy when analyzing item names. These preprocessing steps will ensure that the dataset is clean, focused, and ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7670df0d15d877b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:05.190526Z",
     "start_time": "2024-12-07T01:42:05.087379Z"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_keep = ['BillNo', 'Itemname', 'Country']\n",
    "\n",
    "data = data[columns_to_keep]\n",
    "\n",
    "#Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data['Itemname'] = data['Itemname'].str.strip()\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae48d8727dc7f1",
   "metadata": {},
   "source": [
    "Group Data\n",
    "- Group transaction data by country \n",
    "- Remove \"Unspecified\" country transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f853d44553372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:07.004585Z",
     "start_time": "2024-12-07T01:42:06.949262Z"
    }
   },
   "outputs": [],
   "source": [
    "country_datas = {country: data for country, data in data.groupby('Country')}\n",
    " \n",
    "del country_datas[\"Unspecified\"]\n",
    "\n",
    "for country, data in country_datas.items():\n",
    "        print(f\"Data for {country}:\")\n",
    "        print(data.head()) \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41452519bee441c9",
   "metadata": {},
   "source": [
    "Modify Data to be Transactions for TransactionEncoder\n",
    "- Make transactions by joining items with the same BillNo\n",
    "- Make it so transactions data for each country is setup for the transaction encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a6297f1e4a89a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:09.680234Z",
     "start_time": "2024-12-07T01:42:09.410767Z"
    }
   },
   "outputs": [],
   "source": [
    "country_transactions = {}\n",
    "\n",
    "for country, data in country_datas.items():\n",
    "    country_transactions[country] = data.groupby(['BillNo'])['Itemname'].apply(lambda x: ','.join(x)).reset_index()\n",
    "        \n",
    "for country, transactions in country_transactions.items():\n",
    "    transactions.drop(columns=['BillNo'], inplace=True)\n",
    "    transactions.rename(columns={'Itemname': 'Items'}, inplace=True)\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    print(f\"Transactions for {country}:\")\n",
    "    print(transactions.head()) \n",
    "    print(\"\\n\")\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    country_transactions[country] = transactions['Items'].apply(lambda x: x.split(',')).tolist()\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    print(f\"Transactions for {country}:\")\n",
    "    print(transactions[0]) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c51d98-aed7-44b3-bdac-08c9db3f0f7b",
   "metadata": {},
   "source": [
    "# Generating Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931411d38633657",
   "metadata": {},
   "source": [
    "Make One-Hot Encoded Dataframe for FP-Growth Algorithm\n",
    "- The fpgrowth function from the mlxtend library expects data in a one-hot encoded pandas DataFrame\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefff233771e699a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T01:42:15.219599Z",
     "start_time": "2024-12-07T01:42:12.542240Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "for country, transactions in country_transactions.items():\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    data = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    country_transactions[country] = data\n",
    "    \n",
    "for country, transactions in country_transactions.items():\n",
    "    print(f\"Transactions for {country}:\")\n",
    "    print(transactions.head())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60710d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(country_transactions))\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "fq_itemsets = {}\n",
    "fq_rules = {}\n",
    "# Apply FP-Growth to each country's transactions\n",
    "for country, transactions in country_transactions.items():\n",
    "    # print(country)\n",
    "    frequent_itemsets = fpgrowth(transactions, min_support=0.1, use_colnames=True)\n",
    "    top_itemsets = frequent_itemsets.sort_values(by='support', ascending=False)\n",
    "    rules = association_rules(top_itemsets, metric='confidence', min_threshold=0.2)\n",
    "    fq_itemsets[country] = frequent_itemsets\n",
    "    fq_rules[country] = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63393884",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, itemsets in fq_itemsets.items():\n",
    "    print(f\"Frequent itemsets for {country}\")\n",
    "    print(len(itemsets))\n",
    "    print(itemsets.sort_values(by='support', ascending=False).head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, rules in fq_rules.items():\n",
    "    print(f\"Association rules for {country}\")\n",
    "    print(len(rules))\n",
    "    print(rules.head(5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbb333-7310-4e27-ab07-fd5ae00a9aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9005c4c-c413-45b9-af08-1fe92f4cdd49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "cacba1f63d0a9690f1901f7a6a70bca9f28ac070c74075aae2450729060a64a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
